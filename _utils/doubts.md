# Questions

## LLM
1. Does the prefill phase include the first token generation?
2. What is the request rate in LLM serving?
3. Topics -> Request rate, max number of tokens, Prefix-caching, chunked prefill, inflight batching
4. Parallelism -> PP, TP
5. What is OpenAI server-client setup?

## Device
1. GPUs and architectures
2. Nvidia GPUs

## DeepLearning
1. TensorRT 

## General
