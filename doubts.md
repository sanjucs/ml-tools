# Questions

## LLM
1. Does prefill phase include first token generation?
2. What is request rate in LLM serving?
3. Topics -> Request rate, max number of tokens, Prefix-caching, chunked prefill, inflight batching
4. Parallelism -> PP, TP

## Device
1. GPUs and architectures
2. Nvidia GPUs

## DeepLearning
1. TensorRT 

## General
