# Transformers

Transformers are the core foundational components of popular LLMs such as GPT and LLaMa. The Google Brain team introduced the transformer architecture in 2017. The model consists of an encoder-decoder structure. The encoder converts the input sequence to a latent space representation z, and the decoder converts the representation to an output sequence. The transformer architecture is as shown in Figure 1.

![Transformers](/notes/dl/assets/transformers.png)

*Figure 1: Figure shows different Transformer architecture [Image credits]:[Transformer paper](https://arxiv.org/abs/1706.03762)*

Main components
## Embedding
## Postional encoding
## Scaled Dot Product Attention
### Self attention
### Cross attention
### Multi-head attention
## Feed Forward Networks
## Auto regressive property
## Complexity analysis
## Reference



* [Attention is all you need](https://arxiv.org/abs/1706.03762)